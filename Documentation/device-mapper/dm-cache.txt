* Introduction

dm-cache is a device mapper target written by Joe Thornber, Heinz
Maueslhagen, and Mike Snitzer.

It aims to improve performance of a block device (eg, a spindle) by
dynamically migrating some of its data to a faster, smaller device
(eg, an SSD).

There are various caching solutions out there, for example bcache, we
feel there is a need for a purely device-mapper solution that allows
us to insert this caching at different levels of the dm stack.  For
instance above the data device for a thin-provisioning pool.  Caching
solutions that are integrated more closely with the virtual memory
system should give better performance.

The target reuses the metadata library used in the thin-provisioning
library.

The decision of what and when to migrate data is left to a plug-in
policy module.  Several of these have been written as we experiment,
and we hope other people will contribute others for specific io
scenarios (eg. a vm image server).

* Glossary

- Migration -  Movement of a logical block from one device to the other.
- Promotion -  Migration from slow device to fast device.
- Demotion  -  Migration from fast device to slow device.

* Design

** Sub devices

The target is constructed by passing three devices to it (along with
other params detailed later):

- An origin device (the big, slow one).

- A cache device (the small, fast one).

- A small metadata device.

  Device that records which blocks are in the cache.  Which are dirty,
  and extra hints for use by the policy object.

  This information could be put on the cache device, but having it
  separate allows the volume manager to configure it differently.  eg,
  as a mirror for extra robustness.


** Fixed block size

The origin is divided up into blocks of a fixed size.  This block size
is configurable when you first create the cache.  Typically we've been
using block sizes of 256k - 1024k.

Having a fixed block size simplifies the target a lot.  But it is
something of a compromise.  For instance a small part of a block may
be getting hit a lot (eg, /etc/passwd), yet the whole block will be
promoted to the cache.  So large block sizes are bad, because they
waste cache space.  And small block sizes are bad because they
increase the amount of metadata (both in core and on disk).

** Writeback/writethrough

The cache has these two modes.

If writeback is selected then writes to blocks that are cached will
only go to the cache, and the block will be marked dirty in the
metadata.

If writethrough mode is selected then a write to a cached block will
not complete until has hit both the origin and cache device.  Clean
blocks should remain clean.

A simple cleaner policy is provided, which will clean all dirty blocks
in a cache.  Useful for decommissioning a cache.

** Migration throttling

Migrating data between the origin and cache device uses bandwidth.
The user can set a throttle to prevent more than a certain amount of
migrations occuring at any one time.  Currently we're not taking any
account of normal io traffic going to the devs.  More work needs to be
done here to avoid migrating during those peak io moments.

** Updating on disk metadata

On disk metadata is committed everytime a REQ_SYNC or REQ_FUA bio is
written.  If no such requests are made then commits will occur every
second.  This means the cache behaves like a physical disk that has a
write cache (the same is true of the thin-provisioning target).  If
power is lost you may lose some recent writes.  The metadata should
always be consistent in spite of a crash.

The 'dirty' state for a cache block changes far too frequently for us
to keep updating it on the fly.  So we treat it as a hint.  In normal
operation it will be written when the dm device is suspended.  If the
system crashes all cache blocks will be assumed dirty when restarted.

** per block policy hints

Policy plug-ins can store a chunk of data per cache block.  It's up to
the policy how big this chunk is (please keep it small).  Like the
dirty flags this data is lost if there's a crash so a safe fallback
value should always be possible.

For instance the 'mq' policy, which is currently the default policy,
uses this facility to store the hit count of the cache blocks.  If
there's a crash this information will be lost, which means the cache
may be less efficient until those hit counts are regenerated.

Policy hints effect performance, not correctness.

** Policy messaging

Policies will have different tunables, specific to each one.  So we
need a generic way of getting and setting these.  One way would be
through a sysfs interface; much as we do with a block device's queue
parameters.  Another is to use the device-mapper message facility.
We're using that latter method currently, though don't feel strongly
one way or the other.

** discard bitset resolution

We can avoid copying data during migration if we know the block has
been discarded.  A prime example of this is when mkfs discards the
whole block device.  We store a bitset tracking the discard state of
blocks.  However, we allow this bitset to have a different block size
from the cache blocks.  This is because we need to track the discard
state for all of the origin device (compare with the dirty bitset
which is just for the smaller cache device).

** Target interface

 cache <metadata dev>
       <cache dev>
       <origin dev>
       <block size>
       <#feature args> [<feature arg>]*
       <policy>
       <#policy args>
       [policy args]*

 metadata dev    : fast device holding the persistent metadata
 cache dev	 : fast device holding cached data blocks
 origin dev	 : slow device holding original data blocks
 block size      : cache unit size in sectors
 policy          : the replacement policy to use

 #feature args   : number of feature arguments passed
 feature args    : 'writeback' or 'writethrough' (one or the other).

 #policy args    : an even number of arguments corresponding to
                   key/value pairs passed to the policy.
 policy args     : key/value pairs (eg, 'migration_threshold 1024000')

A policy called 'default' is always registered.  This is an alias for
the policy we currently think is giving best all round performance.

* Example usage

The test suite can be found here:

https://github.com/jthornber/thinp-test-suite

writing table: 0 41943040 cache /dev/mapper/metadata /dev/mapper/ssd /dev/mapper/origin 512 1 writeback cleaner 0

* Policy interface

- Try to keep transactionality out of it.  The core is careful to
  avoid asking about anything that is migrating.  This is a pain, but
  makes it easier to write the policies.

- Mappings are loaded into the policy at construction time.

- Every bio that is mapped by the target is referred to the policy, it
  can give a simple HIT or MISS or issue a migration.

- Currently there's no way for the policy to issue background work,
  eg, start writing back dirty blocks that are soon going to be evicted.

- Because we map bios, rather than requests it's easy for the policy
  to get fooled by many small bios.  For this reason the core target
  issues periodic ticks to the policy.  It's suggested that the policy
  doesn't update states (eg, hit counts) for a block more than once
  for each tick.  [The core ticks by watching bios complete, and so
  trying to see when the io scheduler has let the ios run]


	void (*destroy)(struct dm_cache_policy *p);
	void (*map)(struct dm_cache_policy *p, dm_block_t origin_block, int data_dir,
		    bool can_migrate, bool cheap_copy, struct bio *bio,
		    struct policy_result *result);

	int (*load_mapping)(struct dm_cache_policy *p, dm_block_t oblock, dm_block_t cblock);

	/* must succeed */
	void (*remove_mapping)(struct dm_cache_policy *p, dm_block_t oblock);
	void (*force_mapping)(struct dm_cache_policy *p, dm_block_t current_oblock,
			      dm_block_t new_oblock);

	dm_block_t (*residency)(struct dm_cache_policy *p);
	void (*set_seq_io_threshold)(struct dm_cache_policy *p,
				     unsigned int seq_io_thresh);

	void (*tick)(struct dm_cache_policy *p);

