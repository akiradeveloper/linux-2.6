* Introduction

This document descibes a collection of device-mapper targets that
between them implement thin provisioning and snapshots.

The main highlight of this implementation, compared to the previous
implementation of snapshots, is it allows many virtual devices to be
stored on the same data volume.  Simplifying administration and
allowing sharing of data between volumes (thus reducing disk usage).

Another big feature is support for arbitrary depth of recursive
snapshots (snapshots of snapshots of snapshots ...).  Previous
implementations of snapshots did this by chaining together lookup
tables, and so performance was O(depth).  This implementation uses a
single table so we won't get this degradation with depth (fragmentation
may be an issue however in some scenarios).

Metadata is stored on a separate device from data, this gives the
administrator a bit more freedom.  For instance:

- Improve metadata resilience by storing metadata on a mirrored volume
  but data on a non-mirrored one.

- Improve performance by storing the metadata on an SSD.

* Status

These targets are very much in the EXPERIMENTAL state.  Do not use in
production.

_Do_ experiment with it and give us feedback.  Different use cases
will have different performance characteristics (for example due to
fragmentation of the data volume).  If you find this software is not
performing as expected please mail dm-devel@redhat.com with details and
we'll try our best to improve things for you.

* Cookbook

This section describes some quick recipes for using thin provisioning
using the dmsetup program to control the device-mapper driver
directly.  End users are advised to use a higher level volume manager
such as LVM2.

** Pool device

The pool device ties together the metadata volume and the data volume.
It linearly maps the data volume, and updates the metadata via two
mechanisms:

- Function calls from the thin targets

- device-mapper 'messages' from userland which control creation of new
  virtual devices among other things.

** Setting up a fresh pool device

Setting up a pool device requires a _valid_ metadata device, and a
data device.  If you do not have an existing metadata device you can
make one by zeroing the first 4k to indicate empty metadata.

    dd if=/dev/zero of=$metadata_dev bs=4096 count=1

** reloading a pool table

You may reload a pool's table, indeed this is how the pool is resized.
Its advisable that you reload with a pool target that points to the
same metadata area.

** Using an existing pool device

The pool device must have the same size as the data device, since it
reflects this via a linear mapping.

    dmsetup create pool --table "0 20971520 thin-pool $metadata_dev $data_dev $data_block_size $low_water_mark"

The $data_block_size gives the smallest unit of disk space that can be
allocated at a time.  This is expressed in 512 byte sectors.  People
primarily interested in thin provisioning may want to set this larger
(e.g., 1024).  People doing lots of snapshotting may want it smaller
(e.g., 128).  $data_block_size must be the same for the lifetime of
the metadata device.

The $low_water_mark is expressed in 512 byte sectors, if free space on
the data device drops below this level then a dm event will be sent to
userland.

** Thin provisioning

*** Creating a new thinly provisioned volume

To create a new thin provision volume you must send a message to an
active pool device.

    dmsetup message /dev/mapper/pool 0 "new-thin 0 2097152"

Here '0' is an identifier for the volume (32bit range).  It's up to
the caller to allocate and manage these identifiers.  If there's a
collision the message will fail.

'2097152' is the size of the virtual device.

*** Using a thinp volume

Thin provisioned volumes are instanced using the 'thin' target.

    dmsetup create thin --table "0 2097152 thin /dev/mapper/pool 0"

The last parameter is the 32bit identifier for the thinp device.

** Internal snapshots

*** Creating an internal snapshot

Snapshots are created with another message to the pool.

You _must_ suspend the origin device before creating the snapshot.  If
the origin hasn't been instanced via the 'thin' target then you
may proceed without doing anything.

    dmsetup suspend /dev/mapper/thin
    dmsetup message /dev/mapper/pool 0 "new-snap 1 0"
    dmsetup resume /dev/mapper/thin

Here '1' is an identifier for the volume (32bit range).  '0' is the
identifier for the origin device.

*** Using an internal snapshot

Once created, the user doesn't have to worry about any connection
between the origin and the snapshot.  Indeed the snapshot is no
different from any other thinly provisioned device, and can be
snapshotted itself via the same method.  It's perfectly legal to have
only one of them active, and there's no ordering requirement on
activating/removing them both.

Activate exactly the same way as any other thin provisioned volume.

    dmsetup create snap --table "0 2097152 thin /dev/mapper/pool 1"

** Teardown

Always teardown the pool last.

    dmsetup remove thin
    dmsetup remove snap
    dmsetup remove pool

* Reference

** thin-pool target

*** Constructor

 thin-pool <metadata dev>
           <data dev>
           <data block size in sectors>
           <low water mark (sectors)>
	   [#feature args> [<arg>]*]

 optional feature args:
   - skip_block_zeroing: skips the zeroing of new blocks

*** Status

   <transaction id> <data free space in sectors> <metadata free space in sectors> <held metadata root>

where,

  - transaction id: A 64bit number used by userland to help
    synchronise with metadata from volume managers.

  - held metadata root: The location, in sectors, of the metadata
    root that has been 'held' for userland read access.  '-' indicates
    there is no held root.

  - data free space in sectors: If this drops below the pool's low water
    mark a dm event will be sent to userland.  This event is edge
    triggered, it will occur only once, so volume manager writers should
    register for the event, then double check the status of the target.

*** Messages

 new-thin <dev id>
 new-snap <dev id> <origin id>
 del      <dev id>
 trim     <dev id> <new size in sectors>
 trans-id <current id> <new id>

   Fails if 'current id' is incorrect.

** thin target

 thin <pool dev> <dev id>

 pool dev: the path to the pool (eg, /dev/mapper/my_pool)
 dev id: the internal device identifier (32bit value)

 The device is always implicitly resized to the size of the target.
 If you truncate, any provisioned blocks past the new end of device
 will be lost immediately.

*** Status

 <nr mapped sectors> <highest mapped sector>
